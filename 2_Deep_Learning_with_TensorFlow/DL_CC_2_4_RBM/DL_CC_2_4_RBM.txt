----------------------------------------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------------------------------------------
MODULE 4 - UNSUPERVISED LEARNING
Restricted Boltzmann Machines
- Find the patterns in data in an "unsupervised" manner.
- Shallow neural nets that learn to reconstruct data by themselves
- Unsupervised
- Feature extraction
- Dimensionality Reduction
- Pattern recognition, 
- Recommender systems, 
- Handling missing values, 
- Topic modeling
Deep Belief Network
- Built on top of RBMs
- Solves back-propagation problem, "Local-minima" and "vanishing gradients"
- Solve this by the stacking of multiple RBMs
- Classification
- Image recognition
- A very accurate discriminative classifier - As such, we donâ€™t need a big set of labeled data to train a Deep Belief Network; in fact, a small set works fine because feature extraction is unsupervised by a stack of RBMs.


----------------------------------------------------------------------------------------------------------------------------------------------------
INTRO TO RBMS

"INTRODUCTION"
Restricted Boltzmann Machine (RBM): RBMs are shallow neural nets that learn to reconstruct data by themselves in an unsupervised fashion.

"WHY ARE RBMS IMPORTANT?"
It can automatically extract meaningful features from a given input.

"HOW DOES IT WORK?"
RBM is a 2 layer neural network. Simply, RBM takes the inputs and translates those into a set of binary values that represents them in the hidden layer. Then, these numbers can be translated back to reconstruct the inputs. Through several forward and backward passes, the RBM will be trained, and a trained RBM can reveal which features are the most important ones when detecting patterns.

"WHAT ARE THE APPLICATIONS OF RBM?"
RBM is useful for Collaborative Filtering, dimensionality reduction, classification, regression, feature learning, topic modeling and even Deep Belief Networks

"IS RBM A GENERATIVE OR DISCRIMINATIVE MODEL?"
RBM is a generative model. Let me explain it by first, see what is different between discriminative and generative models:
	- Discriminative: Consider a classification problem in which we want to learn to distinguish between Sedan cars (y = 1) and SUV cars (y = 0), based on some features of cars. Given a training set, an algorithm like logistic regression tries to find a straight lineâ€”that is, a decision boundaryâ€”that separates the suv and sedan.
	- Generative: looking at cars, we can build a model of what Sedan cars look like. Then, looking at SUVs, we can build a separate model of what SUV cars look like. Finally, to classify a new car, we can match the new car against the Sedan model, and match it against the SUV model, to see whether the new car looks more like the SUV or Sedan.

Generative Models specify a probability distribution over a dataset of input vectors. We can do both supervise and unsupervised tasks with generative models:
	- In an unsupervised task, we try to form a model for P(x), where P is the probability given x as an input vector.
	- In the supervised task, we first form a model for P(x|y), where P is the probability of x given y(the label for x). For example, if y = 0 indicates whether a car is a SUV or y = 1 indicates indicate a car is a Sedan, then p(x|y = 0) models the distribution of SUVsâ€™ features, and p(x|y = 1) models the distribution of Sedansâ€™ features. If we manage to find P(x|y) and P(y), then we can use Bayes rule to estimate P(y|x), because:
	ğ‘(ğ‘¦|ğ‘¥)=ğ‘(ğ‘¥|ğ‘¦)ğ‘(ğ‘¦)ğ‘(ğ‘¥)
 


"Now the question is, can we build a generative model, and then use it to create synthetic data by directly sampling from the modeled probability distributions? Lets see."




----------------------------------------------------------------------------------------------------------------------------------------------------
TRAINING RBMS 





----------------------------------------------------------------------------------------------------------------------------------------------------
COLLABORATIVE FILTERING WITH RBM.





----------------------------------------------------------------------------------------------------------------------------------------------------
LABS
4.1-Review-RBMMNIST
"""An RBM has two layers. The first layer of the RBM is called the visible (or input layer). Imagine that our toy example, has only vectors with 7 values, so the visible layer must have j=7 input nodes. The second layer is the hidden layer, which possesses i neurons in our case. Each hidden node can have either 0 or 1 values (i.e., si = 1 or si = 0) with a probability that is a logistic function of the inputs it receives from the other j visible units, called for example, p(si = 1). For our toy sample, we'll use 2 nodes in the hidden layer, so i = 2."""
"https://raw.githubusercontent.com/Gurubux/CognitiveClass-DL/master/2_Deep_Learning_with_TensorFlow/DL_CC_2_4_RBM/RBM%20Model_.png"

#Each node in the first layer also has a bias. We will denote the bias as â€œv_biasâ€ for the visible units. The v_bias is shared among all visible units.
#Here we define the bias of second layer as well. We will denote the bias as â€œh_biasâ€ for the hidden units. The h_bias is shared among all hidden units
v_bias = tf.placeholder("float", [7])
h_bias = tf.placeholder("float", [2])

"""We have to define weights among the input layer and hidden layer nodes. In the weight matrix, the number of rows are equal to the input nodes, and the number of columns are equal to the output nodes. Let W be the Tensor of 7x2 (7 - number of visible neurons, 2 - number of hidden neurons) that represents weights between neurons."""
W = tf.constant(np.random.normal(loc=0.0, scale=1.0, size=(7, 2)).astype(np.float32))

"""What RBM can do after training?
Think RBM as a model that has been trained based on images of a dataset of many SUV and Sedan cars. Also, imagine that the RBM network has only two hidden nodes, one for the weight and, and one for the size of cars, which in a sense, their different configurations represent different cars, one represent SUV cars and one for Sedan. In a training process, through many forward and backward passes, RBM adjust its weights to send a stronger signal to either the SUV node (0, 1) or the Sedan node (1, 0) in the hidden layer, given the pixels of images. Now, given a SUV in hidden layer, which distribution of pixels should we expect? RBM can give you 2 things. First, it encodes your images in hidden layer. Second, it gives you the probability of observing a case, given some hidden values."""
"FORWARD PASS- EXAMPLE"
"""
Phase 1) Forward pass: Input one training sample (one image) X through all visible nodes, and pass it to all hidden nodes. Processing happens in each node in the hidden layer. This computation begins by making stochastic decisions about whether to transmit that input or not (i.e. to determine the state of each hidden layer). At the hidden layer's nodes, X is multiplied by a  ğ‘Šğ‘–ğ‘—  and added to h_bias. The result of those two operations is fed into the sigmoid function, which produces the nodeâ€™s output,  ğ‘(â„ğ‘—) , where j is the unit number.

ğ‘(â„ğ‘—)=ğœ(âˆ‘ğ‘–ğ‘¤ğ‘–ğ‘—ğ‘¥ğ‘–) , where  ğœ()  is the logistic function.

Now lets see what  ğ‘(â„ğ‘—)  represents. In fact, it is the probabilities of the hidden units. And, all values together are called probability distribution. That is, RBM uses inputs x to make predictions about hidden node activations. For example, imagine that the values of  â„ğ‘  for the first training item is [0.51 0.84]. It tells you what is the conditional probability for each hidden neuron to be at Phase 1):

p( â„1  = 1|V) = 0.51
( â„2  = 1|V) = 0.84
As a result, for each row in the training set, a vector/tensor is generated, which in our case it is of size [1x2], and totally n vectors ( ğ‘(â„) =[nx2]).

We then turn unit  â„ğ‘—  on with probability  ğ‘(â„ğ‘—|ğ‘‰) , and turn it off with probability  1âˆ’ğ‘(â„ğ‘—|ğ‘‰) .

Therefore, the conditional probability of a configuration of h given v (for a training sample) is:

ğ‘(ğ¡âˆ£ğ¯)=âˆğ‘—=0ğ»ğ‘(â„ğ‘—âˆ£ğ¯)
 
Now, sample a hidden activation vector h from this probability distribution  ğ‘(â„ğ‘—) . That is, we sample the activation vector from the probability distribution of hidden layer values.

Before we go further, let's look at a toy example for one case out of all input. Assume that we have a trained RBM, and a very simple input vector such as [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0], lets see what would be the output of forward pass:
"""

sess = tf.Session()
X = tf.constant([[1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]])
v_state = X
print ("Input: ", sess.run(v_state))

h_bias = tf.constant([0.1, 0.1])
print ("hb: ", sess.run(h_bias))
print ("w: ", sess.run(W))

# Calculate the probabilities of turning the hidden units on:
h_prob = tf.nn.sigmoid(tf.matmul(v_state, W) + h_bias)  #probabilities of the hidden units
print ("p(h|v): ", sess.run(h_prob))

print(sess.run(tf.shape(h_prob)),sess.run(tf.random_uniform(tf.shape(h_prob))))
print("tf.sign(h_prob - tf.random_uniform(tf.shape(h_prob)))--> ",sess.run(tf.sign(h_prob - tf.random_uniform(tf.shape(h_prob)))))
# Draw samples from the distribution:
h_state = tf.nn.relu(tf.sign(h_prob - tf.random_uniform(tf.shape(h_prob)))) #states
print ("h0 states:", sess.run(h_state))
>>>
Input:  [[1. 0. 0. 1. 0. 0. 0.]]
hb:  [0.1 0.1]
w:  [[ 0.6706064   0.60742325]
 [-0.13637309 -0.96916723]
 [-0.9486356   0.7280032 ]
 [ 0.21499676  0.94692266]
 [ 0.21739139 -0.02056309]
 [-0.3884501  -1.3380241 ]
 [-0.3197343  -1.3242232 ]]
p(h|v):  [[0.7282186  0.83947754]]
[1 2] [[0.73173964 0.54673374]]
tf.sign(h_prob - tf.random_uniform(tf.shape(h_prob)))-->  [[-1.  1.]]
h0 states: [[0. 1.]]

"BACKWARD PASS (RECONSTRUCTION)- EXAMPLE"
"""Phase 2) Backward Pass (Reconstruction): The RBM reconstructs data by making several forward and backward passes between the visible and hidden layers.

So, in the second phase (i.e. reconstruction phase), the samples from the hidden layer (i.e. h) play the role of input. That is, h becomes the input in the backward pass. The same weight matrix and visible layer biases are used to go through the sigmoid function. The produced output is a reconstruction which is an approximation of the original input."""
vb = tf.constant([0.1, 0.2, 0.1, 0.1, 0.1, 0.2, 0.1])
print ("b: ", sess.run(vb))

#   Ïƒ[ ([[0. 1.]] * W) + vb ]
v_prob = sess.run(tf.nn.sigmoid(tf.matmul(h_state, tf.transpose(W)) + vb))
print ("p(viâˆ£h): ", v_prob)

v_state = tf.nn.relu(tf.sign(v_prob - tf.random_uniform(tf.shape(v_prob))))
print ("v probability states: ", sess.run(v_state))

>>>
b:  [0.1 0.2 0.1 0.1 0.1 0.2 0.1]
p(viâˆ£h):  [[0.73767453 0.38724554 0.71199036 0.32375377 0.4309596  0.08817312
  0.28142536]]
v probability states:  [[1. 1. 1. 0. 0. 0. 0.]]

inp = sess.run(X)
print(inp)
print(v_prob[0])
v_probability = 1
for elm, p in zip(inp[0],v_prob[0]) :
    if elm ==1:
        v_probability *= p
    else:
        v_probability *= (1-p)
v_probability

[[1. 0. 0. 1. 0. 0. 0.]]
[0.73767453 0.38724554 0.71199036 0.32375377 0.4309596  0.08817312
 0.28142536]
0.01571449808851681
--------------------------------------------------------------------------------------------------------------------------------------
\MNIST
#MNIST images have 784 pixels, so the visible layer must have 784 input nodes. For our case, we'll use 50 nodes in the hidden layer, so i = 50.
vb = tf.placeholder("float", [784])
hb = tf.placeholder("float", [50])

#Let W be the Tensor of 784x50 (784 - number of visible neurons, 50 - number of hidden neurons) that represents weights between the neurons.
W = tf.placeholder("float", [784, 50])

#visible layer
v0_state = tf.placeholder("float", [None, 784])

#hidden layer
h0_prob = tf.nn.sigmoid(tf.matmul(v0_state, W) + hb)  #probabilities of the hidden units
h0_state = tf.nn.relu(tf.sign(h0_prob - tf.random_uniform(tf.shape(h0_prob)))) #sample_h_given_X

#define reconstruction part
v1_prob = tf.nn.sigmoid(tf.matmul(h0_state, tf.transpose(W)) + vb) 
v1_state = tf.nn.relu(tf.sign(v1_prob - tf.random_uniform(tf.shape(v1_prob)))) #sample_v_given_h



\What is objective function?
"Goal": Maximize the likelihood of our data being drawn from that distribution
"Calculate error":
	In each epoch, we compute the "error" as a sum of the squared difference between step 1 and step n, e.g the error shows the difference between the data and its reconstruction.

"Note": tf.reduce_mean computes the mean of elements across dimensions of a tensor.

This cannot be easily done by typical gradient descent (SGD), so we can use another approach, which has 2 steps:
	- Gibbs Sampling
	- Contrastive Divergence

"GIBBS SAMPLING"
First, given an input vector v we are using p(h|v) for prediction of the hidden values h.
ğ‘(â„|ğ‘£)=ğ‘ ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘(ğ‘‹âŠ—ğ‘Š+â„ğ‘) 
h0 = sampleProb(h0)
Then, knowing the hidden values, we use p(v|h) for reconstructing of new input values v.

ğ‘(ğ‘£|â„)=ğ‘ ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘(â„0âŠ—ğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘ ğ‘ğ‘œğ‘ ğ‘’(ğ‘Š)+ğ‘£ğ‘) 
ğ‘£1=ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’ğ‘ƒğ‘Ÿğ‘œğ‘(ğ‘£1)  (Sample v given h)
This process is repeated k times. After k iterations we obtain an other input vector vk which was recreated from original input values v0 or X.


"CONTRASTIVE DIVERGENCE (CD-K)"
The update of the weight matrix is done during the Contrastive Divergence step.
Vectors v0 and vk are used to calculate the activation probabilities for hidden values h0 and hk. The difference between the outer products of those probabilities with input vectors v0 and vk results in the update matrix:

Î”ğ‘Š=ğ‘£0âŠ—â„0âˆ’ğ‘£ğ‘˜âŠ—â„ğ‘˜ 

Contrastive Divergence is actually matrix of values that is computed and used to adjust values of the W matrix. Changing W incrementally leads to training of W values. Then on each step (epoch), W is updated to a new value W' through the equation below:

ğ‘Šâ€²=ğ‘Š+ğ‘ğ‘™ğ‘â„ğ‘âˆ—Î”ğ‘Š



h1_prob = tf.nn.sigmoid(tf.matmul(v1_state, W) + hb)
h1_state = tf.nn.relu(tf.sign(h1_prob - tf.random_uniform(tf.shape(h1_prob)))) #sample_h_given_X

alpha = 0.01
W_Delta = tf.matmul(tf.transpose(v0_state), h0_prob) - tf.matmul(tf.transpose(v1_state), h1_prob)
update_w = W + alpha * W_Delta
update_vb = vb + alpha * tf.reduce_mean(v0_state - v1_state, 0)
update_hb = hb + alpha * tf.reduce_mean(h0_state - h1_state, 0)


cur_w = np.zeros([784, 50], np.float32)
cur_vb = np.zeros([784], np.float32)
cur_hb = np.zeros([50], np.float32)
prv_w = np.zeros([784, 50], np.float32)
prv_vb = np.zeros([784], np.float32)
prv_hb = np.zeros([50], np.float32)
sess = tf.Session()
init = tf.global_variables_initializer()
sess.run(init)

sess.run(err, feed_dict={v0_state: trX, W: prv_w, vb: prv_vb, hb: prv_hb})
0.48159346

#Parameters
epochs = 5
batchsize = 100
weights = []
errors = []

for epoch in range(epochs):
    for start, end in zip( range(0, len(trX), batchsize), range(batchsize, len(trX), batchsize)):
        batch = trX[start:end]
        cur_w = sess.run(update_w, feed_dict={ v0_state: batch, W: prv_w, vb: prv_vb, hb: prv_hb})
        cur_vb = sess.run(update_vb, feed_dict={v0_state: batch, W: prv_w, vb: prv_vb, hb: prv_hb})
        cur_hb = sess.run(update_hb, feed_dict={ v0_state: batch, W: prv_w, vb: prv_vb, hb: prv_hb})
        prv_w = cur_w
        prv_vb = cur_vb
        prv_hb = cur_hb
        if start % 10000 == 0:
            errors.append(sess.run(err, feed_dict={v0_state: trX, W: cur_w, vb: cur_vb, hb: cur_hb}))
            weights.append(cur_w)
    print ('Epoch: %d' % epoch,'reconstruction error: %f' % errors[-1])
plt.plot(errors)
plt.xlabel("Batch Number")
plt.ylabel("Error")
plt.show()
>>>
Epoch: 0 reconstruction error: 0.092520
Epoch: 1 reconstruction error: 0.090862
Epoch: 2 reconstruction error: 0.089657
Epoch: 3 reconstruction error: 0.089712
Epoch: 4 reconstruction error: 0.089439


uw = weights[-1].T
print (uw) # a weight matrix of shape (50,784)


\Learned features
We can take each hidden unit and visualize the connections between that hidden unit and each element in the input vector. In our case, we have 50 hidden units. Lets visualize those.
Let`s plot the current weights: tile_raster_images helps in generating an easy to grasp image from a set of samples or weights. It transform the uw (with one flattened image per row of size 784), into an array (of size  25Ã—20 ) in which images are reshaped and laid out like tiles on a floor.

tile_raster_images(X=cur_w.T, img_shape=(28, 28), tile_shape=(5, 10), tile_spacing=(1, 1))
import matplotlib.pyplot as plt
from PIL import Image
%matplotlib inline
image = Image.fromarray(tile_raster_images(X=cur_w.T, img_shape=(28, 28) ,tile_shape=(5, 10), tile_spacing=(1, 1)))
### Plot image
plt.rcParams['figure.figsize'] = (18.0, 18.0)
imgplot = plt.imshow(image)
imgplot.set_cmap('gray')  























----------------------------------------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------------------------------------------
4.2-Review-CollaborativeFilteringwithRBM
	MovieID	Title			 					Genres
0	1		Toy Story (1995)					Animation|Children`s|Comedy
1	2		Jumanji (1995)						Adventure|Children`s|Fantasy
2	3		Grumpier Old Men (1995)				Comedy|Romance
3	4		Waiting to Exhale (1995)			Comedy|Drama
4	5		Father of the Bride Part II (1995)	Comedy

	UserID		MovieID	Rating	Timestamp
0	1	1193	5				978300760
1	1	661		3				978302109
2	1	914		3				978301968
3	1	3408	4				978300275
4	1	2355	5				978824291

\Formatting the Data
#Now, we can start formatting the data into input for the RBM. We're going to store the normalized users ratings into as a matrix of user-rating called trX, and normalize the values.
user_rating_df = ratings_df.pivot(index='UserID', columns='MovieID', values='Rating')
user_rating_df.head()
6040 rows Ã— 3706 columns
MovieID	1	2	3	4	5	6	7	8	9	10	...	3943	3944	3945	3946	3947	3948	3949	3950	3951	3952
UserID																					
1		5.0	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	...	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN
2		NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	...	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN
3		NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	...	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN
4		NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	...	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN
5		NaN	NaN	NaN	NaN	NaN	2.0	NaN	NaN	NaN	NaN	...	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN

\normalize
norm_user_rating_df = user_rating_df.fillna(0) / 5.0
trX = norm_user_rating_df.values
trX[0:5]

array([[1., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]])

\Setting the Model's Parameters

hiddenUnits = 20
visibleUnits =  len(user_rating_df.columns)
vb = tf.placeholder("float", [visibleUnits]) #Number of unique movies
hb = tf.placeholder("float", [hiddenUnits]) #Number of features we're going to learn
W = tf.placeholder("float", [visibleUnits, hiddenUnits])




\"MAIN STEPS FP and BP"
#######################################################################
\Phase 1: Input Processing
v0 = tf.placeholder("float", [None, visibleUnits])
_h0 = tf.nn.sigmoid(tf.matmul(v0, W) + hb)
h0 = tf.nn.relu(tf.sign(_h0 - tf.random_uniform(tf.shape(_h0))))


\Phase 2: Reconstruction
_v1 = tf.nn.sigmoid(tf.matmul(h0, tf.transpose(W)) + vb) 
v1 = tf.nn.relu(tf.sign(_v1 - tf.random_uniform(tf.shape(_v1))))

h1 = tf.nn.sigmoid(tf.matmul(v1, W) + hb)
#######################################################################

#Learning rate
alpha = 1.0

#Create the gradients GIBBS SAMPLING ğ‘(â„|ğ‘£)=ğ‘ ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘(ğ‘‹âŠ—ğ‘Š+â„ğ‘)   #ğ‘(ğ‘£|â„)=ğ‘ ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘(â„0âŠ—ğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘ ğ‘ğ‘œğ‘ ğ‘’(ğ‘Š)+ğ‘£ğ‘) 
w_pos_grad = tf.matmul(tf.transpose(v0), h0)
w_neg_grad = tf.matmul(tf.transpose(v1), h1)

#Calculate the Contrastive Divergence to maximize # Î”ğ‘Š = (ğ‘£0 âŠ— â„0) âˆ’ (ğ‘£ğ‘˜ âŠ— â„ğ‘˜) - 
CD = (w_pos_grad - w_neg_grad) / tf.to_float(tf.shape(v0)[0])

#Create methods to update the weights and biases # ğ‘Šâ€² = ğ‘Š + ğ‘ğ‘™ğ‘â„ğ‘ âˆ— Î”ğ‘Š
update_w = W + alpha * CD
update_vb = vb + alpha * tf.reduce_mean(v0 - v1, 0)
update_hb = hb + alpha * tf.reduce_mean(h0 - h1, 0)

err = v0 - v1
err_sum = tf.reduce_mean(err * err)

#Current weight
cur_w = np.zeros([visibleUnits, hiddenUnits], np.float32)
#Current visible unit biases
cur_vb = np.zeros([visibleUnits], np.float32)
#Current hidden unit biases
cur_hb = np.zeros([hiddenUnits], np.float32)
#Previous weight
prv_w = np.zeros([visibleUnits, hiddenUnits], np.float32)
#Previous visible unit biases
prv_vb = np.zeros([visibleUnits], np.float32)
#Previous hidden unit biases
prv_hb = np.zeros([hiddenUnits], np.float32)
sess = tf.Session()
sess.run(tf.global_variables_initializer())


epochs = 15
batchsize = 100
errors = []
for i in range(epochs):
    for start, end in zip( range(0, len(trX), batchsize), range(batchsize, len(trX), batchsize)):
        batch = trX[start:end]
        cur_w = sess.run(update_w, feed_dict={v0: batch, W: prv_w, vb: prv_vb, hb: prv_hb})
        cur_vb = sess.run(update_vb, feed_dict={v0: batch, W: prv_w, vb: prv_vb, hb: prv_hb})
        cur_nb = sess.run(update_hb, feed_dict={v0: batch, W: prv_w, vb: prv_vb, hb: prv_hb})
        prv_w = cur_w
        prv_vb = cur_vb
        prv_hb = cur_hb
    errors.append(sess.run(err_sum, feed_dict={v0: trX, W: cur_w, vb: cur_vb, hb: cur_hb}))
    print (errors[-1])
plt.plot(errors)
plt.ylabel('Error')
plt.xlabel('Epoch')
plt.show()


\Recommendation
mock_user_id = 215

#Selecting the input user
inputUser = trX[mock_user_id-1].reshape(1, -1)
inputUser[0:5]
array([[0.8, 0. , 0. , ..., 0. , 0. , 0. ]])

#Feeding in the user and reconstructing the input
hh0 = tf.nn.sigmoid(tf.matmul(v0, W) + hb)
vv1 = tf.nn.sigmoid(tf.matmul(hh0, tf.transpose(W)) + vb)
feed = sess.run(hh0, feed_dict={ v0: inputUser, W: prv_w, hb: prv_hb})
rec = sess.run(vv1, feed_dict={ hh0: feed, W: prv_w, vb: prv_vb})
print(rec)
[[0.31401148 0.22032845 0.02421106 ... 0.00336228 0.00272663 0.02197457]]


scored_movies_df_mock = movies_df[movies_df['MovieID'].isin(user_rating_df.columns)]
scored_movies_df_mock = scored_movies_df_mock.assign(RecommendationScore = rec[0])
scored_movies_df_mock.sort_values(["RecommendationScore"], ascending=False).head(20)

		MovieID	Title	 			 								Genres 		                   RecommendationScore
257		260		Star Wars: Episode IV - A New Hope (1977)			Action|Adventure|Fantasy|Sci-Fi	0.984106
1178	1196	Star Wars: Episode V - The Empire Strikes Back...	Action|Adventure|Drama|Sci-Fi|War	0.935710
2502	2571	Matrix, The (1999)		                            Action|Sci-Fi|Thriller	0.930792
1192	1210	Star Wars: Episode VI - Return of the Jedi (1983)	Action|Adventure|Romance|Sci-Fi|War	0.889082
585		589		Terminator 2: Judgment Day (1991)	                Action|Sci-Fi|Thriller	0.839085
1539	1580	Men in Black (1997)	                                Action|Adventure|Comedy|Sci-Fi	0.783183
2559	2628	Star Wars: Episode I - The Phantom Menace (1999)	Action|Adventure|Fantasy|Sci-Fi	0.766976
476		480		Jurassic Park (1993)	                            Action|Adventure|Sci-Fi	0.757520
1335	1356	Star Trek: First Contact (1996)	                    Action|Adventure|Sci-Fi	0.744073
1180	1198	Raiders of the Lost Ark (1981)	                    Action|Adventure	0.733858
1271	1291	Indiana Jones and the Last Crusade (1989)	        Action|Adventure	0.667820
1353	1374	Star Trek: The Wrath of Khan (1982)	                Action|Adventure|Sci-Fi	0.652664
1081	1097	E.T. the Extra-Terrestrial (1982)	                Children`s|Drama|Fantasy|Sci-Fi	0.610945
2847	2916	Total Recall (1990)	                                Action|Adventure|Sci-Fi|Thriller	0.607223
1220	1240	Terminator, The (1984)	                            Action|Sci-Fi|Thriller	0.606743
1355	1376	Star Trek IV: The Voyage Home (1986)	            Action|Adventure|Sci-Fi	0.569480
1959	2028	Saving Private Ryan (1998)							Action|Drama|War	0.554293
1179	1197	Princess Bride, The (1987)							Action|Adventure|Comedy|Romance	0.553741
770		780		Independence Day (ID4) (1996)						Action|Sci-Fi|War	0.553683
1351	1372	Star Trek VI: The Undiscovered Country (1991)		Action|Adventure|Sci-Fi	0.551598


#Now, we can find all the movies that our mock user has watched before:
movies_df_mock = ratings_df[ratings_df['UserID'] == mock_user_id]
movies_df_mock.head()
	UserID	MovieID	Rating	Timestamp
31603	215	3793	5	977099259
31604	215	1	4	979174987
31605	215	1197	5	976899663
31606	215	2302	5	976899718
31607	215	2167	5	976899770


#In the next cell, we merge all the movies that our mock users has watched with the predicted scores based on his historical data:
#Merging movies_df with ratings_df by MovieID
merged_df_mock = scored_movies_df_mock.merge(movies_df_mock, on='MovieID', how='outer')
merged_df_mock.sort_values(["RecommendationScore"], ascending=False).head(20)

	     MovieID	Title	                                            Genres	                          RecommendationScore	UserID	Rating	Timestamp
253	    260	    	Star Wars: Episode IV - A New Hope (1977)	        Action|Adventure|Fantasy|Sci-Fi	    0.984106			215.0	5.0		976899190.0
1106	1196 		Star Wars: Episode V - The Empire Strikes Back...	Action|Adventure|Drama|Sci-Fi|War   0.935710			\NaN		NaN		NaN
2374	2571		Matrix, The (1999)	                                Action|Sci-Fi|Thriller	            0.930792			\NaN		NaN		NaN
1120	1210		Star Wars: Episode VI - Return of the Jedi (1983)	Action|Adventure|Romance|Sci-Fi|War	0.889082			215.0	5.0		976899689.0
575	    589	    	Terminator 2: Judgment Day (1991)	                Action|Sci-Fi|Thriller	            0.839085			\NaN		NaN		NaN
1449	1580		Men in Black (1997)	                                Action|Adventure|Comedy|Sci-Fi	    0.783183			\NaN		NaN		NaN



"As you can see, there are some movies that user has not watched yet and has high score based on our model. So, we can recommend them to the user."
"This is the end of the module. If you want, you can try to change the parameters in the code -- adding more units to the hidden layer, changing the loss functions or maybe something else to see if it changes anything. Does the model perform better? Does it take longer to compute?"
----------------------------------------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------------------------------------------
QUIZ
@Select the True statement about Restricted means in RBM?
 - It is a Boltzmann machine, but with no connections between nodes in the same layer  
 - Each node in the first layer has a bias.  
 - The RBM reconstructs data by making several forward and backward passes between the visible and hidden layers.  
 - At the hidden layer`s nodes, X is multiplied by a W (weight matrix) and added to h_bias.

@
- The objective function is to maximize the likelihood of our data being drawn from the reconstructed data distribution  
- The Negative phase of RBM decreases the probability of samples generated by the model.  
- Contrastive Divergence (CD) is used to approximate the negative phase of RBM.  
- The Positive phase of RBM increases the probability of training data.

@How RBM compares to PCA?
 Both can regenerate input data.

@How many layers has a RBM (Restricted Boltzmann Machine)?
2

@What things we can do with unsupervised learning?
 	- Data dimensionality reduction  
 	- Object recognition  
 	- Feature extraction  
 	- Pattern recognition
