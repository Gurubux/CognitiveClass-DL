{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.bigdatauniversity.com\"><img src = \"https://ibm.box.com/shared/static/jvcqp2iy2jlx2b32rmzdt0tx8lvxgzkp.png\" width = 300, align = \"center\"></a>\n",
    "\n",
    "\n",
    "# <center> Text generation using RNN/LSTM (Character-level)</center>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<font size = 3><strong>In this notebook you will learn the How to use TensorFlow for create a Recurrent Neural Network</strong></font>\n",
    "<br>    \n",
    "- <a href=\"#intro\">Introduction</a>\n",
    "<br>\n",
    "- <p><a href=\"#arch\">Architectures</a></p>\n",
    "    - <a href=\"#lstm\">Long Short-Term Memory Model (LSTM)</a>\n",
    "\n",
    "- <p><a href=\"#build\">Building a LSTM with TensorFlow</a></p>\n",
    "</div>\n",
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code implements a Recurrent Neural Network with LSTM/RNN units for training/sampling from character-level language models. In other words, the model takes a text file as input and trains the RNN network that learns to predict the next character in a sequence.  \n",
    "The RNN can then be used to generate text character by character that will look like the original training data. \n",
    "\n",
    "This code is based on this [blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), and the code is an step-by-step implimentation of the [character-level implimentation](https://github.com/crazydonkey200/tensorflow-char-rnn).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import the requiered libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import codecs\n",
    "import os\n",
    "import collections\n",
    "from six.moves import cPickle\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader\n",
    "The following cell is a class that help to read data from input file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextLoader():\n",
    "    def __init__(self, data_dir, batch_size, seq_length, encoding='utf-8'):\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "        self.encoding = encoding\n",
    "\n",
    "        input_file = os.path.join(data_dir, \"input.txt\")\n",
    "        vocab_file = os.path.join(data_dir, \"vocab.pkl\")\n",
    "        tensor_file = os.path.join(data_dir, \"data.npy\")\n",
    "\n",
    "        if not (os.path.exists(vocab_file) and os.path.exists(tensor_file)):\n",
    "            print(\"reading text file\")\n",
    "            self.preprocess(input_file, vocab_file, tensor_file)\n",
    "        else:\n",
    "            print(\"loading preprocessed files\")\n",
    "            self.load_preprocessed(vocab_file, tensor_file)\n",
    "        self.create_batches()\n",
    "        self.reset_batch_pointer()\n",
    "\n",
    "    def preprocess(self, input_file, vocab_file, tensor_file):\n",
    "        with codecs.open(input_file, \"r\", encoding=self.encoding) as f:\n",
    "            data = f.read()\n",
    "        counter = collections.Counter(data)\n",
    "        count_pairs = sorted(counter.items(), key=lambda x: -x[1])\n",
    "        self.chars, _ = zip(*count_pairs)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n",
    "        with open(vocab_file, 'wb') as f:\n",
    "            cPickle.dump(self.chars, f)\n",
    "        self.tensor = np.array(list(map(self.vocab.get, data)))\n",
    "        np.save(tensor_file, self.tensor)\n",
    "\n",
    "    def load_preprocessed(self, vocab_file, tensor_file):\n",
    "        with open(vocab_file, 'rb') as f:\n",
    "            self.chars = cPickle.load(f)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n",
    "        self.tensor = np.load(tensor_file)\n",
    "        self.num_batches = int(self.tensor.size / (self.batch_size * self.seq_length))\n",
    "\n",
    "    def create_batches(self):\n",
    "        self.num_batches = int(self.tensor.size / (self.batch_size * self.seq_length))\n",
    "\n",
    "        # When the data (tensor) is too small, let's give them a better error message\n",
    "        if self.num_batches==0:\n",
    "            assert False, \"Not enough data. Make seq_length and batch_size small.\"\n",
    "\n",
    "        self.tensor = self.tensor[:self.num_batches * self.batch_size * self.seq_length]\n",
    "        xdata = self.tensor\n",
    "        ydata = np.copy(self.tensor)\n",
    "        ydata[:-1] = xdata[1:]\n",
    "        ydata[-1] = xdata[0]\n",
    "        self.x_batches = np.split(xdata.reshape(self.batch_size, -1), self.num_batches, 1)\n",
    "        self.y_batches = np.split(ydata.reshape(self.batch_size, -1), self.num_batches, 1)\n",
    "\n",
    "\n",
    "    def next_batch(self):\n",
    "        x, y = self.x_batches[self.pointer], self.y_batches[self.pointer]\n",
    "        self.pointer += 1\n",
    "        return x, y\n",
    "\n",
    "    def reset_batch_pointer(self):\n",
    "        self.pointer = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "#### Batch, number_of_batch, batch_size and seq_length\n",
    "what is batch, number_of_batch, batch_size and seq_length in the charcter level example?  \n",
    "\n",
    "Lets assume the input is this sentence: '__here is an example__'. Then:\n",
    "- txt_length = 18  \n",
    "- seq_length = 3  \n",
    "- batch_size = 2  \n",
    "- number_of_batchs = 18/3*2 = 3\n",
    "- batch = array (['h','e','r'],['e',' ','i'])\n",
    "- sample Seq = 'her'  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now, lets look at a real dataset, with real parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 50 # RNN sequence length\n",
    "batch_size = 60  # minibatch size, i.e. size of data in each epoch\n",
    "num_epochs = 125 # you should change it to 50 if you want to see a relatively good results\n",
    "learning_rate = 0.002\n",
    "decay_rate = 0.97\n",
    "rnn_size = 128 # size of RNN hidden state (output dimension)\n",
    "num_layers = 2 #number of layers in the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download the input file, and print a part of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-07 06:12:04 URL:https://public.boxcloud.com/d/1/b1!nHwbrSBQmU64BHHoFilA6jQyBZBrCZFFnYvaMyP4dit6yCeZK5zM9oVG-0Rn_SUEC3C5IgDoTzJDvSBDiXF8xWYuw93xSAYp74XogqjjaTB3eEDvcwUSwA-pwHKeYcZDKHkEiRM-RvG5TFUGJZksasGIFIrPJT19A7dCwRNvquLBKlwPOt64PFL-A6EuftWBtkf1gkg6N2KFKi2b63VjX43YyEIyDtrtYBBmZ2JVnypM9u8jrYM9aFL0dzEsEL2UBavLvwqoXuB7FsehuGY1ISPkTMMjKbBaevW7wdnI-Bs5pGVqlb24zZ-aAEezlFHwDQUTU-deb1aSMLGXNgBGCYbODrTEpFD2QRXzm74UGWucFYHA2SEon61-JuvXO-HmJxCHEDBroKfGPOcMbisKdgCaFfDajzqvRgTE1DrBvYXFlBwEsXRluTTUewRrGwHQd0g1GSW06yvhGmbNG6u-JxZ-QdJ9Uuph_FJVRC1XDLhKwdN4iTK3uFZKm1znzbETrla2Q6E_ObxmxcwyJLGwr6aB1KhqT22vIedu1NY0BtyGQdO16yF8pbsEg_6Aekqa1cp8uuwBriDJHjIAG-e0eSbv5kyIa8RClThn-P1tGk8r9vm4iHCB224wOh3ylm1XyrdOdZ2j_QY-HtILbe4iA7N6KNBcr-1He9xa9-HiF3Fgb1yepo9ocGNfnW1w_Se-B68mtMsy6pVvCeFPx1jt5g96AgzO2F6KO4m5PQQ8ZP_QpgiYo674fqQMvjJKkkLt6ZAbeWL5HnsbJ668r0rjPD0E-oA2LrXQ9a6gFn_IgeVK290OfDgWRA1rpSVepUsmecZtk3lrtNXu-J8K1wtAQU6eYbxsDLOd42oCPVpMLq9mVP2ZU6JpVCE4aljGHU-d15OwrGbhAWrC-y0NYB97JIiPXYKr1ypYHipoM1d5lWant7yUWc-jJJxwj2lnAgvt_UdqdDGO0rFTsPVCAqpH-I7nlnqD3uPSvyJVRQMGlpaw4pcdrj930DxZV4NdNbbtieddS6OfFSznFpcuSWQp-gXLbiDMtOFhreJDnwvC2wauZZ1EfXvt08ih0ZzSvc9ZllTZq_IulBisn8lJrBXBDh0b0R_dN1k9ClZRJqyPY1dGMrDsu6rWaLxu00ivtI0bnhhZ7B6KvBaBPjLbJPU1AP9bS8B3RGY4pBr5IoOQY26cEtT2R2UATihjP_ieTX5F5pc6_IikqEPS8UqywCiE99eifqC5SfFUGjNdiCsadf4mFx1t0reIwLwSJZ4efCo41384ND3AWoyLdO2OGbgd73QqWWpuPPLAYCq-H-pg8sv78oHlXEVcdmQEBOIQgqvxo1DL8DNJ_KbQbgOBx5jBZESil4CT6jeFoDbSCbBv2Vnv1VgBl28./download [1115393/1115393] -> \"input.txt\" [1]\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!wget -nv -O input.txt https://ibm.box.com/shared/static/a3f9e9mbpup09toq35ut7ke3l3lf03hg.txt \n",
    "with open('input.txt', 'r') as f:\n",
    "    read_data = f.read()\n",
    "    print (read_data[0:100])\n",
    "f.closed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can read the data at batches using the __TextLoader__ class. It will convert the characters to numbers, and represent each sequence as a vector in batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading text file\n",
      "vocabulary size: 65\n",
      "Characters: (' ', 'e', 't', 'o', 'a', 'h', 's', 'r', 'n', 'i', '\\n', 'l', 'd', 'u', 'm', 'y', ',', 'w', 'f', 'c', 'g', 'I', 'b', 'p', ':', '.', 'A', 'v', 'k', 'T', \"'\", 'E', 'O', 'N', 'R', 'S', 'L', 'C', ';', 'W', 'U', 'H', 'M', 'B', '?', 'G', '!', 'D', '-', 'F', 'Y', 'P', 'K', 'V', 'j', 'q', 'x', 'z', 'J', 'Q', 'Z', 'X', '3', '&', '$')\n",
      "vocab number of 'F': 49\n",
      "Character sequences (first batch): [[49  9  7 ...  1  4  7]\n",
      " [19  4 14 ... 14  9 20]\n",
      " [ 8 20 10 ...  8 10 18]\n",
      " ...\n",
      " [21  2  0 ...  0 21  0]\n",
      " [ 9  7  7 ...  0  2  3]\n",
      " [ 3  7  0 ...  5  9 23]]\n"
     ]
    }
   ],
   "source": [
    "data_loader = TextLoader('', batch_size, seq_length)\n",
    "vocab_size = data_loader.vocab_size\n",
    "print (\"vocabulary size:\" ,data_loader.vocab_size)\n",
    "print (\"Characters:\" ,data_loader.chars)\n",
    "print (\"vocab number of 'F':\",data_loader.vocab['F'])\n",
    "print (\"Character sequences (first batch):\", data_loader.x_batches[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[49,  9,  7, ...,  1,  4,  7],\n",
       "       [19,  4, 14, ..., 14,  9, 20],\n",
       "       [ 8, 20, 10, ...,  8, 10, 18],\n",
       "       ...,\n",
       "       [21,  2,  0, ...,  0, 21,  0],\n",
       "       [ 9,  7,  7, ...,  0,  2,  3],\n",
       "       [ 3,  7,  0, ...,  5,  9, 23]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = data_loader.next_batch()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 50)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape  #batch_size =60, seq_length=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, __y__ is the next character for each character in __x__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9,  7,  6, ...,  4,  7,  0],\n",
       "       [ 4, 14, 22, ...,  9, 20,  5],\n",
       "       [20, 10, 29, ..., 10, 18,  4],\n",
       "       ...,\n",
       "       [ 2,  0,  6, ..., 21,  0,  6],\n",
       "       [ 7,  7,  4, ...,  2,  3,  0],\n",
       "       [ 7,  0, 33, ...,  9, 23,  0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Architecture\n",
    "Each LSTM cell has 5 parts:\n",
    "1. Input\n",
    "2. prv_state\n",
    "3. prv_output\n",
    "4. new_state\n",
    "5. new_output\n",
    "\n",
    "\n",
    "- Each LSTM cell has an input layre, which its size is 128 units in our case. The input vector's dimension also is 128, which is the dimensionality of embedding vector, so called, dimension size of W2V/embedding, for each character/word.\n",
    "- Each LSTM cell has a hidden layer, where there are some hidden units. The argument n_hidden=128 of BasicLSTMCell is the number of hidden units of the LSTM (inside A). It keeps the size of the output and state vector. It is also known as, rnn_size, num_units, num_hidden_units, and LSTM size\n",
    "- An LSTM keeps two pieces of information as it propagates through time: \n",
    "    - __hidden state__ vector: Each LSTM cell accept a vector, called __hidden state__ vector, of size n_hidden=128, and its value is returned to the LSTM cell in the next step. The __hidden state__ vector; which is the memory of the LSTM, accumulates using its (forget, input, and output) gates through time. \"num_units\" is equivalant to \"size of RNN hidden state\". number of hidden units is the dimensianality of the output (= dimesianality of the state) of the LSTM cell.\n",
    "    - __previous time-step output__: For each LSTM cell that we initialize, we need to supply a value (128 in this case) for the hidden dimension, or as some people like to call it, the number of units in the LSTM cell. \n",
    "\n",
    "\n",
    "#### num_layers = 2 \n",
    "- number of layers in the RNN, is defined by num_layers\n",
    "- An input of MultiRNNCell is __cells__ which is list of RNNCells that will be composed in this order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining stacked RNN Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__BasicRNNCell__ is the most basic RNN cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = tf.contrib.rnn.BasicRNNCell(rnn_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a two layer cell\n",
    "stacked_cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hidden state size\n",
    "stacked_cell.output_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__state__ varibale keeps output and new_state of the LSTM, so it is a touple of size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_cell.state_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder:0' shape=(60, 50) dtype=int32>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = tf.placeholder(tf.int32, [batch_size, seq_length])# a 60x50\n",
    "input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and target data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder_1:0' shape=(60, 50) dtype=int32>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = tf.placeholder(tf.int32, [batch_size, seq_length]) # a 60x50\n",
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The memory state of the network is initialized with a vector of zeros and gets updated after reading each character.\n",
    "\n",
    "__BasicRNNCell.zero_state(batch_size, dtype)__ Return zero-filled state tensor(s). In this function, batch_size\n",
    "representing the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = stacked_cell.zero_state(batch_size, tf.float32) #why batch_size ? 60x128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the value of the input_data again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[49,  9,  7, ...,  1,  4,  7],\n",
       "       [19,  4, 14, ..., 14,  9, 20],\n",
       "       [ 8, 20, 10, ...,  8, 10, 18],\n",
       "       ...,\n",
       "       [21,  2,  0, ...,  0, 21,  0],\n",
       "       [ 9,  7,  7, ...,  0,  2,  3],\n",
       "       [ 3,  7,  0, ...,  5,  9, 23]], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session = tf.Session()\n",
    "feed_dict={input_data:x, targets:y}\n",
    "session.run(input_data, feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "In this section, we build a 128-dim vector for each character. As we have 60 batches, and 50 character in each sequence, it will generate a [60,50,128] matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Notice:__ The function `tf.get_variable()` is used to share a variable and to initialize it in one place. `tf.get_variable()` is used to get or create a variable instead of a direct call to `tf.Variable`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('rnnlm', reuse=False):\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size]) #128x65\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [vocab_size]) # 1x65)\n",
    "    #with tf.device(\"/cpu:0\"):\n",
    "        \n",
    "    # embedding variable is initialized randomely\n",
    "    embedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])  #65x128\n",
    "\n",
    "    # embedding_lookup goes to each row of input_data, and for each character in the row, finds the correspond vector in embedding\n",
    "    # it creates a 60*50*[1*128] matrix\n",
    "    # so, the first elemnt of em, is a matrix of 50x128, which each row of it is vector representing that character\n",
    "    em = tf.nn.embedding_lookup(embedding, input_data) # em is 60x50x[1*128]\n",
    "    # split: Splits a tensor into sub tensors.\n",
    "    # syntax:  tf.split(split_dim, num_split, value, name='split')\n",
    "    # it will split the 60x50x[1x128] matrix into 50 matrix of 60x[1*128]\n",
    "    inputs = tf.split(em, seq_length, 1)\n",
    "    # It will convert the list to 50 matrix of [60x128]\n",
    "    inputs = [tf.squeeze(input_, [1]) for input_ in inputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the __embedding__, __em__, and __inputs__ variabbles:\n",
    "\n",
    "Embedding variable is initialized with random values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.04614291, -0.02960823,  0.0063169 , ...,  0.11470546,\n",
       "        -0.10217441,  0.05214652],\n",
       "       [ 0.08279179, -0.08833045, -0.07790183, ...,  0.03377399,\n",
       "        -0.04801913, -0.16013035],\n",
       "       [ 0.05258195, -0.15809166,  0.04950447, ...,  0.13140906,\n",
       "         0.11996482, -0.1329734 ],\n",
       "       ...,\n",
       "       [-0.16071162, -0.10666423, -0.00079408, ..., -0.15032162,\n",
       "         0.04642892, -0.13005601],\n",
       "       [ 0.01128501,  0.08314578, -0.11720043, ...,  0.02054811,\n",
       "        -0.07040203,  0.04369602],\n",
       "       [-0.17473726, -0.15456505, -0.01225297, ...,  0.00220197,\n",
       "         0.15279149,  0.02776738]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "#print embedding.shape\n",
    "session.run(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first elemnt of em, is a matrix of 50x128, which each row of it is vector representing that character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 50, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.10554545, -0.16812152,  0.01216133, ...,  0.11528336,\n",
       "        -0.07852158, -0.01221472],\n",
       "       [-0.11470744, -0.15552402, -0.14366409, ..., -0.17374665,\n",
       "        -0.12416849, -0.15780833],\n",
       "       [ 0.05744968,  0.02870575,  0.1754369 , ...,  0.14357094,\n",
       "         0.12638263, -0.07668042],\n",
       "       ...,\n",
       "       [ 0.08279179, -0.08833045, -0.07790183, ...,  0.03377399,\n",
       "        -0.04801913, -0.16013035],\n",
       "       [-0.02258928,  0.15794356, -0.04799069, ...,  0.14750548,\n",
       "        -0.10110426,  0.10911076],\n",
       "       [ 0.05744968,  0.02870575,  0.1754369 , ...,  0.14357094,\n",
       "         0.12638263, -0.07668042]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em = tf.nn.embedding_lookup(embedding, input_data)\n",
    "emp = session.run(em,feed_dict={input_data:x})\n",
    "print (emp.shape)\n",
    "emp[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider each sequence as a sentence of length 50 characters, then, the first item in __inputs__ is a [60x128] vector which represents the first characters of 60 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'Squeeze:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_1:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_2:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_3:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_4:0' shape=(60, 128) dtype=float32>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tf.split(em, seq_length, 1)\n",
    "inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n",
    "inputs[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feeding a batch of 50 sequence to a RNN:\n",
    "\n",
    "The feeding process for iputs is as following:\n",
    "\n",
    "- Step 1:  first character of each of the 50 sentences (in a batch) is entered in parallel.  \n",
    "- Step 2:  second character of each of the 50 sentences is input in parallel. \n",
    "- Step n: nth character of each of the 50 sentences is input in parallel.  \n",
    "\n",
    "The parallelism is only for efficiency.  Each character in a batch is handled in parallel,  but the network sees one character of a sequence at a time and does the computations accordingly. All the computations involving the characters of all sequences in a batch at a given time step are done in parallel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.10554545, -0.16812152,  0.01216133, ...,  0.11528336,\n",
       "        -0.07852158, -0.01221472],\n",
       "       [ 0.03564095, -0.0624556 ,  0.15932487, ...,  0.00431924,\n",
       "        -0.0953864 , -0.10208672],\n",
       "       [ 0.1430466 ,  0.14693628, -0.01841798, ...,  0.02720042,\n",
       "        -0.07882518, -0.06115231],\n",
       "       ...,\n",
       "       [-0.04446128, -0.15686648,  0.09138377, ...,  0.02649145,\n",
       "         0.16158132, -0.08332046],\n",
       "       [-0.11470744, -0.15552402, -0.14366409, ..., -0.17374665,\n",
       "        -0.12416849, -0.15780833],\n",
       "       [ 0.14121865,  0.12101246, -0.00519672, ..., -0.11795744,\n",
       "         0.14243336, -0.14977753]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(inputs[0],feed_dict={input_data:x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feeding the RNN with one batch, we can check the new output and new state of network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_98:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_99:0' shape=(60, 128) dtype=float32>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#outputs is 50x[60*128]\n",
    "outputs, new_state = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, initial_state, stacked_cell, loop_function=None, scope='rnnlm')\n",
    "new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_1:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_3:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_5:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_7:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_9:0' shape=(60, 128) dtype=float32>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the output of network after feeding it with first batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.05066605, -0.04608541, -0.01056738, ..., -0.03304396,\n",
       "        -0.10072914, -0.03902555],\n",
       "       [-0.0337635 , -0.03605497, -0.06998975, ...,  0.01858982,\n",
       "         0.00051874, -0.05548633],\n",
       "       [ 0.13607761, -0.01967493, -0.01823763, ..., -0.09705225,\n",
       "        -0.04838498,  0.11088895],\n",
       "       ...,\n",
       "       [-0.14785346, -0.01738407, -0.01186183, ...,  0.06737781,\n",
       "        -0.11410083, -0.15171535],\n",
       "       [-0.01698176, -0.02727038,  0.05145252, ...,  0.00281979,\n",
       "        -0.00283647,  0.03170124],\n",
       "       [ 0.00944696,  0.05065018, -0.05664184, ..., -0.09679979,\n",
       "        -0.06439876, -0.02070861]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_output = outputs[0]\n",
    "session.run(tf.global_variables_initializer())\n",
    "session.run(first_output,feed_dict={input_data:x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it was explained, __outputs__ variable is a 50x[60x128] tensor. We need to reshape it back to [60x50x128] to be able to calculate the probablity of the next character using the softmax. The __softmax_w__ shape is [rnn_size, vocab_size],whihc is [128x65] in our case. Threfore, we have a fully connected layer on top of LSTM cells, which help us to decode the next charachter. We can use the __softmax(output * softmax_w + softmax_b)__ for this purpose. The shape of the matrixis would be:\n",
    "\n",
    "softmax([60x50x128]x[128x65]+[1x65]) = [60x50x65]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do it step-by-step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape:0' shape=(3000, 128) dtype=float32>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tf.reshape(tf.concat( outputs,1), [-1, rnn_size])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add:0' shape=(3000, 65) dtype=float32>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Softmax:0' shape=(3000, 65) dtype=float32>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = tf.nn.softmax(logits)\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the probablity of the next chracter in all batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01438482, 0.01908016, 0.01191478, ..., 0.01639259, 0.01351416,\n",
       "        0.0165209 ],\n",
       "       [0.01376203, 0.01865158, 0.01237575, ..., 0.01639377, 0.01320747,\n",
       "        0.01702691],\n",
       "       [0.01757197, 0.01770249, 0.01252973, ..., 0.01480446, 0.01275755,\n",
       "        0.01840238],\n",
       "       ...,\n",
       "       [0.02059537, 0.01841748, 0.01605616, ..., 0.01206754, 0.00946766,\n",
       "        0.01482594],\n",
       "       [0.01261081, 0.0147527 , 0.01091655, ..., 0.01689394, 0.0123901 ,\n",
       "        0.01424157],\n",
       "       [0.01058824, 0.01984143, 0.01517595, ..., 0.01651856, 0.0125467 ,\n",
       "        0.01908481]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "session.run(probs,feed_dict={input_data:x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are in the position to calculate the cost of training with __loss function__, and keep feedng the network to learn it. But, the question is: what the LSTM networks learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'rnnlm/softmax_w:0' shape=(128, 65) dtype=float32_ref>,\n",
       " <tf.Variable 'rnnlm/softmax_b:0' shape=(65,) dtype=float32_ref>,\n",
       " <tf.Variable 'rnnlm/embedding:0' shape=(65, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/kernel:0' shape=(256, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/bias:0' shape=(128,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_clip =5.\n",
    "tvars = tf.trainable_variables()\n",
    "tvars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All together\n",
    "Now, let's put all of parts together in a class, and train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel():\n",
    "    def __init__(self,sample=False):\n",
    "        rnn_size = 128 # size of RNN hidden state vector\n",
    "        batch_size = 60 # minibatch size, i.e. size of dataset in each epoch\n",
    "        seq_length = 50 # RNN sequence length\n",
    "        num_layers = 2 # number of layers in the RNN\n",
    "        vocab_size = 65\n",
    "        grad_clip = 5.\n",
    "        if sample:\n",
    "            print(\">> sample mode:\")\n",
    "            batch_size = 1\n",
    "            seq_length = 1\n",
    "        # The core of the model consists of an LSTM cell that processes one char at a time and computes probabilities of the possible continuations of the char. \n",
    "        basic_cell = tf.contrib.rnn.BasicRNNCell(rnn_size)\n",
    "        # model.cell.state_size is (128, 128)\n",
    "        self.stacked_cell = tf.contrib.rnn.MultiRNNCell([basic_cell] * num_layers)\n",
    "\n",
    "        self.input_data = tf.placeholder(tf.int32, [batch_size, seq_length], name=\"input_data\")\n",
    "        self.targets = tf.placeholder(tf.int32, [batch_size, seq_length], name=\"targets\")\n",
    "        # Initial state of the LSTM memory.\n",
    "        # The memory state of the network is initialized with a vector of zeros and gets updated after reading each char. \n",
    "        self.initial_state = stacked_cell.zero_state(batch_size, tf.float32) #why batch_size\n",
    "\n",
    "        with tf.variable_scope('rnnlm_class1'):\n",
    "            softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size]) #128x65\n",
    "            softmax_b = tf.get_variable(\"softmax_b\", [vocab_size]) # 1x65\n",
    "            with tf.device(\"/cpu:0\"):\n",
    "                embedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])  #65x128\n",
    "                inputs = tf.split(tf.nn.embedding_lookup(embedding, self.input_data), seq_length, 1)\n",
    "                inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n",
    "                #inputs = tf.split(em, seq_length, 1)\n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "        # The value of state is updated after processing each batch of chars.\n",
    "        outputs, last_state = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, self.initial_state, self.stacked_cell, loop_function=None, scope='rnnlm_class1')\n",
    "        output = tf.reshape(tf.concat(outputs,1), [-1, rnn_size])\n",
    "        self.logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        self.probs = tf.nn.softmax(self.logits)\n",
    "        loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([self.logits],\n",
    "                [tf.reshape(self.targets, [-1])],\n",
    "                [tf.ones([batch_size * seq_length])],\n",
    "                vocab_size)\n",
    "        self.cost = tf.reduce_sum(loss) / batch_size / seq_length\n",
    "        self.final_state = last_state\n",
    "        self.lr = tf.Variable(0.0, trainable=False)\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars),grad_clip)\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    \n",
    "    def sample(self, sess, chars, vocab, num=200, prime='The ', sampling_type=1):\n",
    "        state = sess.run(self.stacked_cell.zero_state(1, tf.float32))\n",
    "        #print state\n",
    "        for char in prime[:-1]:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[char]\n",
    "            feed = {self.input_data: x, self.initial_state:state}\n",
    "            [state] = sess.run([self.final_state], feed)\n",
    "\n",
    "        def weighted_pick(weights):\n",
    "            t = np.cumsum(weights)\n",
    "            s = np.sum(weights)\n",
    "            return(int(np.searchsorted(t, np.random.rand(1)*s)))\n",
    "\n",
    "        ret = prime\n",
    "        char = prime[-1]\n",
    "        for n in range(num):\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[char]\n",
    "            feed = {self.input_data: x, self.initial_state:state}\n",
    "            [probs, state] = sess.run([self.probs, self.final_state], feed)\n",
    "            p = probs[0]\n",
    "\n",
    "            if sampling_type == 0:\n",
    "                sample = np.argmax(p)\n",
    "            elif sampling_type == 2:\n",
    "                if char == ' ':\n",
    "                    sample = weighted_pick(p)\n",
    "                else:\n",
    "                    sample = np.argmax(p)\n",
    "            else: # sampling_type == 1 default:\n",
    "                sample = weighted_pick(p)\n",
    "\n",
    "            pred = chars[sample]\n",
    "            ret += pred\n",
    "            char = pred\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the LSTM object\n",
    "Now we create a LSTM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"rnn\"):\n",
    "    model = LSTMModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train usinng LSTMModel class\n",
    "We can train our model through feeding batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370/46375 (epoch 0), train_loss = 1.919, time/batch = 0.143\n",
      ">> sample mode:\n",
      "The has\n",
      "doth profol a onter andor thee, Conere: a't! I\n",
      "----------------------------------\n",
      "741/46375 (epoch 1), train_loss = 1.754, time/batch = 0.117\n",
      ">> sample mode:\n",
      "The breath a lost--nock'd much,\n",
      "The she knee that shou\n",
      "----------------------------------\n",
      "1112/46375 (epoch 2), train_loss = 1.676, time/batch = 0.096\n",
      ">> sample mode:\n",
      "The confencer, and gloson, they bropher me the good sh\n",
      "----------------------------------\n",
      "1483/46375 (epoch 3), train_loss = 1.636, time/batch = 0.236\n",
      ">> sample mode:\n",
      "The laisontbalight: wars, for what wo give my dukes sa\n",
      "----------------------------------\n",
      "1854/46375 (epoch 4), train_loss = 1.611, time/batch = 0.159\n",
      ">> sample mode:\n",
      "The furd rurder; onguers, lalashing this consuties boa\n",
      "----------------------------------\n",
      "2225/46375 (epoch 5), train_loss = 1.594, time/batch = 0.120\n",
      ">> sample mode:\n",
      "The so for thiusidel upon,\n",
      "Were shall hare for to-say \n",
      "----------------------------------\n",
      "2596/46375 (epoch 6), train_loss = 1.581, time/batch = 0.067\n",
      ">> sample mode:\n",
      "The leave to my love?\n",
      "\n",
      "PETES:\n",
      "Last thou send yet, what\n",
      "----------------------------------\n",
      "2967/46375 (epoch 7), train_loss = 1.569, time/batch = 0.060\n",
      ">> sample mode:\n",
      "The heaven him feace to like have of eye;\n",
      "And beave tr\n",
      "----------------------------------\n",
      "3338/46375 (epoch 8), train_loss = 1.558, time/batch = 0.055\n",
      ">> sample mode:\n",
      "The cannot to came your gurst of rawning a theak a bur\n",
      "----------------------------------\n",
      "3709/46375 (epoch 9), train_loss = 1.548, time/batch = 0.059\n",
      ">> sample mode:\n",
      "The in the ingline for enemy,\n",
      "Hanher to not:\n",
      "Natar you\n",
      "----------------------------------\n",
      "4080/46375 (epoch 10), train_loss = 1.539, time/batch = 0.067\n",
      ">> sample mode:\n",
      "The but used not the ween.\n",
      "\n",
      "GRUMIO:\n",
      "Grief, for that na\n",
      "----------------------------------\n",
      "4451/46375 (epoch 11), train_loss = 1.531, time/batch = 0.057\n",
      ">> sample mode:\n",
      "The unkind her.\n",
      "\n",
      "HENRY BOLINGBROKE:\n",
      "Yet if so flame be\n",
      "----------------------------------\n",
      "4822/46375 (epoch 12), train_loss = 1.524, time/batch = 0.076\n",
      ">> sample mode:\n",
      "The of end\n",
      "Our state Riven to his wile,\n",
      "His gentle wit\n",
      "----------------------------------\n",
      "5193/46375 (epoch 13), train_loss = 1.518, time/batch = 0.085\n",
      ">> sample mode:\n",
      "The love,\n",
      "By en't should\n",
      "That you nops;\n",
      "From to the ca\n",
      "----------------------------------\n",
      "5564/46375 (epoch 14), train_loss = 1.513, time/batch = 0.078\n",
      ">> sample mode:\n",
      "The all of the May, be been a stom?\n",
      "\n",
      "QUEEN MARGARET:\n",
      "N\n",
      "----------------------------------\n",
      "5935/46375 (epoch 15), train_loss = 1.509, time/batch = 0.065\n",
      ">> sample mode:\n",
      "The reeather of no hade yearf part,\n",
      "And to content to \n",
      "----------------------------------\n",
      "6306/46375 (epoch 16), train_loss = 1.505, time/batch = 0.057\n",
      ">> sample mode:\n",
      "The rook to himss and breed of a maracunt myself;\n",
      "Nor \n",
      "----------------------------------\n",
      "6677/46375 (epoch 17), train_loss = 1.502, time/batch = 0.081\n",
      ">> sample mode:\n",
      "The but thou hast in thy ham,\n",
      "And,\n",
      "Oxford, lady thou w\n",
      "----------------------------------\n",
      "7048/46375 (epoch 18), train_loss = 1.498, time/batch = 0.059\n",
      ">> sample mode:\n",
      "The on them on.\n",
      "\n",
      "Fir, feel eif out but that crack, Ric\n",
      "----------------------------------\n",
      "7419/46375 (epoch 19), train_loss = 1.495, time/batch = 0.091\n",
      ">> sample mode:\n",
      "The Ilast I'll have heard with no.\n",
      "In Mised;\n",
      "But thou \n",
      "----------------------------------\n",
      "7790/46375 (epoch 20), train_loss = 1.491, time/batch = 0.060\n",
      ">> sample mode:\n",
      "The ky almo?\n",
      "\n",
      "GLOUCESTER:\n",
      "Go, and thou scares be my de\n",
      "----------------------------------\n",
      "8161/46375 (epoch 21), train_loss = 1.488, time/batch = 0.068\n",
      ">> sample mode:\n",
      "The our saper\n",
      "As in this: make exprep best blood, no g\n",
      "----------------------------------\n",
      "8532/46375 (epoch 22), train_loss = 1.486, time/batch = 0.099\n",
      ">> sample mode:\n",
      "The retethops thought no: I from,\n",
      "Have to London up an\n",
      "----------------------------------\n",
      "8903/46375 (epoch 23), train_loss = 1.483, time/batch = 0.059\n",
      ">> sample mode:\n",
      "The child is precomend.\n",
      "\n",
      "GLOUCESTER:\n",
      "Appeal\n",
      "Tissle\n",
      "Hav\n",
      "----------------------------------\n",
      "9274/46375 (epoch 24), train_loss = 1.481, time/batch = 0.114\n",
      ">> sample mode:\n",
      "The have horsed with thou mayor: but by my mind out th\n",
      "----------------------------------\n",
      "9645/46375 (epoch 25), train_loss = 1.479, time/batch = 0.063\n",
      ">> sample mode:\n",
      "The prinches, for his true.\n",
      "\n",
      "ETRSEY:\n",
      "And proned, give \n",
      "----------------------------------\n",
      "10016/46375 (epoch 26), train_loss = 1.477, time/batch = 0.062\n",
      ">> sample mode:\n",
      "The Give me to butter good her soul sin,\n",
      "Stankled tort\n",
      "----------------------------------\n",
      "10387/46375 (epoch 27), train_loss = 1.476, time/batch = 0.074\n",
      ">> sample mode:\n",
      "The scart you than and how almort,\n",
      "Having, which I nev\n",
      "----------------------------------\n",
      "10758/46375 (epoch 28), train_loss = 1.474, time/batch = 0.073\n",
      ">> sample mode:\n",
      "The are warish, frame too love, above him your lovely \n",
      "----------------------------------\n",
      "11129/46375 (epoch 29), train_loss = 1.473, time/batch = 0.054\n",
      ">> sample mode:\n",
      "The of a noble hand of micks but friends, sir; which t\n",
      "----------------------------------\n",
      "11500/46375 (epoch 30), train_loss = 1.471, time/batch = 0.063\n",
      ">> sample mode:\n",
      "The word it.\n",
      "\n",
      "MENENIUS:\n",
      "No, good father, come of justi\n",
      "----------------------------------\n",
      "11871/46375 (epoch 31), train_loss = 1.470, time/batch = 0.058\n",
      ">> sample mode:\n",
      "The high-moording Duke of put: but 'tis within that tr\n",
      "----------------------------------\n",
      "12242/46375 (epoch 32), train_loss = 1.469, time/batch = 0.059\n",
      ">> sample mode:\n",
      "The hence, boy my man I shall of it in him.\n",
      "\n",
      "GREMIO:\n",
      "I\n",
      "----------------------------------\n",
      "12613/46375 (epoch 33), train_loss = 1.467, time/batch = 0.098\n",
      ">> sample mode:\n",
      "The stonest be, but so, who\n",
      "prople the beex, by our ri\n",
      "----------------------------------\n",
      "12984/46375 (epoch 34), train_loss = 1.466, time/batch = 0.064\n",
      ">> sample mode:\n",
      "The provers:\n",
      "I didn of sut thou with and than the sill\n",
      "----------------------------------\n",
      "13355/46375 (epoch 35), train_loss = 1.465, time/batch = 0.062\n",
      ">> sample mode:\n",
      "The horron too.\n",
      "\n",
      "CUMILIO:\n",
      "Rescrack\n",
      "my chice that so?\n",
      "\n",
      "\n",
      "----------------------------------\n",
      "13726/46375 (epoch 36), train_loss = 1.464, time/batch = 0.055\n",
      ">> sample mode:\n",
      "The great kiss, yield be into thy sort, the people,\n",
      "Th\n",
      "----------------------------------\n",
      "14097/46375 (epoch 37), train_loss = 1.463, time/batch = 0.059\n",
      ">> sample mode:\n",
      "The was the wife; Boldoner bids of the mittrected, Bap\n",
      "----------------------------------\n",
      "14468/46375 (epoch 38), train_loss = 1.461, time/batch = 0.054\n",
      ">> sample mode:\n",
      "The chose; a-look--what news answers why thou lo, husb\n",
      "----------------------------------\n",
      "14839/46375 (epoch 39), train_loss = 1.460, time/batch = 0.047\n",
      ">> sample mode:\n",
      "The with propuleh a king ascondence upon the trumpet w\n",
      "----------------------------------\n",
      "15210/46375 (epoch 40), train_loss = 1.459, time/batch = 0.057\n",
      ">> sample mode:\n",
      "The that, we may we will in Lord, call, Pire the lipe\n",
      "\n",
      "----------------------------------\n",
      "15581/46375 (epoch 41), train_loss = 1.458, time/batch = 0.062\n",
      ">> sample mode:\n",
      "The would you remainess of master sir?\n",
      "\n",
      "ESCALUS:\n",
      "Foe u\n",
      "----------------------------------\n",
      "15952/46375 (epoch 42), train_loss = 1.457, time/batch = 0.050\n",
      ">> sample mode:\n",
      "The not me.\n",
      "Thy king, nor night partick against, the s\n",
      "----------------------------------\n",
      "16323/46375 (epoch 43), train_loss = 1.456, time/batch = 0.053\n",
      ">> sample mode:\n",
      "The is the canst a mail;\n",
      "Hear my leasures of the small\n",
      "----------------------------------\n",
      "16694/46375 (epoch 44), train_loss = 1.455, time/batch = 0.077\n",
      ">> sample mode:\n",
      "The I think that they Jachdes: by conceit me out incli\n",
      "----------------------------------\n",
      "17065/46375 (epoch 45), train_loss = 1.453, time/batch = 0.058\n",
      ">> sample mode:\n",
      "The Elysing one hath removed!\n",
      "Good mine.\n",
      "\n",
      "YORK:\n",
      "Even h\n",
      "----------------------------------\n",
      "17436/46375 (epoch 46), train_loss = 1.452, time/batch = 0.069\n",
      ">> sample mode:\n",
      "The a meed thee so countermine east with is all only m\n",
      "----------------------------------\n",
      "17807/46375 (epoch 47), train_loss = 1.451, time/batch = 0.062\n",
      ">> sample mode:\n",
      "The left to her concerely, soe are-day, if my true com\n",
      "----------------------------------\n",
      "18178/46375 (epoch 48), train_loss = 1.450, time/batch = 0.064\n",
      ">> sample mode:\n",
      "The king mother-full grate? he will tame.\n",
      "\n",
      "HERMIONE:\n",
      "Y\n",
      "----------------------------------\n",
      "18549/46375 (epoch 49), train_loss = 1.449, time/batch = 0.060\n",
      ">> sample mode:\n",
      "The ourself!\n",
      "O, yet I hear the majestice.\n",
      "\n",
      "CAMILLO:\n",
      "Ha\n",
      "----------------------------------\n",
      "18920/46375 (epoch 50), train_loss = 1.448, time/batch = 0.057\n",
      ">> sample mode:\n",
      "The master, she scarning.\n",
      "no shall have myskled,\n",
      "Hard,\n",
      "----------------------------------\n",
      "19291/46375 (epoch 51), train_loss = 1.447, time/batch = 0.058\n",
      ">> sample mode:\n",
      "The Edward me at the virgin of one smalia.\n",
      "Come; and n\n",
      "----------------------------------\n",
      "19662/46375 (epoch 52), train_loss = 1.446, time/batch = 0.073\n",
      ">> sample mode:\n",
      "The please he As you, off you will of every will, sinc\n",
      "----------------------------------\n",
      "20033/46375 (epoch 53), train_loss = 1.446, time/batch = 0.062\n",
      ">> sample mode:\n",
      "The you were not fiftery at, in bloo-man, pood as her,\n",
      "----------------------------------\n",
      "20404/46375 (epoch 54), train_loss = 1.445, time/batch = 0.061\n",
      ">> sample mode:\n",
      "The answer of the\n",
      "rut roon.\n",
      "Thines thanks?\n",
      "\n",
      "Second Lan\n",
      "----------------------------------\n",
      "20775/46375 (epoch 55), train_loss = 1.444, time/batch = 0.058\n",
      ">> sample mode:\n",
      "The kiving he'll provot thee are eycle all purchase do\n",
      "----------------------------------\n",
      "21146/46375 (epoch 56), train_loss = 1.443, time/batch = 0.060\n",
      ">> sample mode:\n",
      "The had the ohen dispates, dodgeds my enough, let them\n",
      "----------------------------------\n",
      "21517/46375 (epoch 57), train_loss = 1.442, time/batch = 0.059\n",
      ">> sample mode:\n",
      "The part them to disping, and enemy as voice, what the\n",
      "----------------------------------\n",
      "21888/46375 (epoch 58), train_loss = 1.441, time/batch = 0.061\n",
      ">> sample mode:\n",
      "The kill of my livence made me as far his mind than ar\n",
      "----------------------------------\n",
      "22259/46375 (epoch 59), train_loss = 1.441, time/batch = 0.067\n",
      ">> sample mode:\n",
      "The would not the mabs. Then grow herements:\n",
      "I would i\n",
      "----------------------------------\n",
      "22630/46375 (epoch 60), train_loss = 1.440, time/batch = 0.063\n",
      ">> sample mode:\n",
      "The pity\n",
      "Within thy own shall be wither of good sir, a\n",
      "----------------------------------\n",
      "23001/46375 (epoch 61), train_loss = 1.439, time/batch = 0.065\n",
      ">> sample mode:\n",
      "The well cast divine; I am no slips, and dreadful of y\n",
      "----------------------------------\n",
      "23372/46375 (epoch 62), train_loss = 1.439, time/batch = 0.056\n",
      ">> sample mode:\n",
      "The with brother's deser, that and known to falluses u\n",
      "----------------------------------\n",
      "23743/46375 (epoch 63), train_loss = 1.438, time/batch = 0.049\n",
      ">> sample mode:\n",
      "The store.\n",
      "\n",
      "COMINIUS:\n",
      "I have done?\n",
      "Say so, some busine\n",
      "----------------------------------\n",
      "24114/46375 (epoch 64), train_loss = 1.437, time/batch = 0.070\n",
      ">> sample mode:\n",
      "The shall be cauqleads, know'--Form edder you shall be\n",
      "----------------------------------\n",
      "24485/46375 (epoch 65), train_loss = 1.437, time/batch = 0.060\n",
      ">> sample mode:\n",
      "The appoint of will office in the fiand to any to Surm\n",
      "----------------------------------\n",
      "24856/46375 (epoch 66), train_loss = 1.436, time/batch = 0.060\n",
      ">> sample mode:\n",
      "The he had\n",
      "to acquent here?\n",
      "\n",
      "EXTON:\n",
      "Lord Shill,\n",
      "God, w\n",
      "----------------------------------\n",
      "25227/46375 (epoch 67), train_loss = 1.436, time/batch = 0.059\n",
      ">> sample mode:\n",
      "The boy as colour to Laniolies' unreat me eyes? Honest\n",
      "----------------------------------\n",
      "25598/46375 (epoch 68), train_loss = 1.435, time/batch = 0.079\n",
      ">> sample mode:\n",
      "The whom that Mier in his attinger of thyselves,\n",
      "And p\n",
      "----------------------------------\n",
      "25969/46375 (epoch 69), train_loss = 1.435, time/batch = 0.106\n",
      ">> sample mode:\n",
      "The power dishonedion in jes o't-like about him hath t\n",
      "----------------------------------\n",
      "26340/46375 (epoch 70), train_loss = 1.434, time/batch = 0.057\n",
      ">> sample mode:\n",
      "The are Tybalt, that known his part:\n",
      "Alas,\n",
      "You are lik\n",
      "----------------------------------\n",
      "26711/46375 (epoch 71), train_loss = 1.434, time/batch = 0.057\n",
      ">> sample mode:\n",
      "The honour\n",
      "Than that thou regot she arm with-riset.\n",
      "\n",
      "G\n",
      "----------------------------------\n",
      "27082/46375 (epoch 72), train_loss = 1.433, time/batch = 0.088\n",
      ">> sample mode:\n",
      "The of him.\n",
      "\n",
      "SICINIUS:\n",
      "What would not before use. Go n\n",
      "----------------------------------\n",
      "27453/46375 (epoch 73), train_loss = 1.433, time/batch = 0.075\n",
      ">> sample mode:\n",
      "The pulther lo, as you weep insuppored,\n",
      "Yet is the hit\n",
      "----------------------------------\n",
      "27824/46375 (epoch 74), train_loss = 1.432, time/batch = 0.081\n",
      ">> sample mode:\n",
      "The king; you have you at help, to birge my rushmed\n",
      "Wi\n",
      "----------------------------------\n",
      "28195/46375 (epoch 75), train_loss = 1.432, time/batch = 0.056\n",
      ">> sample mode:\n",
      "The kill and or comfort and hither of vass\n",
      "Ad not the \n",
      "----------------------------------\n",
      "28566/46375 (epoch 76), train_loss = 1.431, time/batch = 0.065\n",
      ">> sample mode:\n",
      "The kepart your firm but any can poor dost muth frown \n",
      "----------------------------------\n",
      "28937/46375 (epoch 77), train_loss = 1.431, time/batch = 0.111\n",
      ">> sample mode:\n",
      "The hear, end was too, tilite, thou careful me.\n",
      "\n",
      "COMIN\n",
      "----------------------------------\n",
      "29308/46375 (epoch 78), train_loss = 1.431, time/batch = 0.057\n",
      ">> sample mode:\n",
      "The with his man and all, to be up then absence we'll \n",
      "----------------------------------\n",
      "29679/46375 (epoch 79), train_loss = 1.430, time/batch = 0.069\n",
      ">> sample mode:\n",
      "The let me chop, and behim, ison that musides Romeo, n\n",
      "----------------------------------\n",
      "30050/46375 (epoch 80), train_loss = 1.430, time/batch = 0.056\n",
      ">> sample mode:\n",
      "The old?\n",
      "\n",
      "LEONTES:\n",
      "O loun:\n",
      "Nay, most ighoution halfs w\n",
      "----------------------------------\n",
      "30421/46375 (epoch 81), train_loss = 1.429, time/batch = 0.103\n",
      ">> sample mode:\n",
      "The gentle power as way\n",
      "I\n",
      "saw a tendered unto your man\n",
      "----------------------------------\n",
      "30792/46375 (epoch 82), train_loss = 1.429, time/batch = 0.065\n",
      ">> sample mode:\n",
      "The know\n",
      "The mind about purrant away, and they dier Mo\n",
      "----------------------------------\n",
      "31163/46375 (epoch 83), train_loss = 1.429, time/batch = 0.089\n",
      ">> sample mode:\n",
      "The oat.\n",
      "\n",
      "Boy: the kept dovoy Buckinger,\n",
      "He being thou\n",
      "----------------------------------\n",
      "31534/46375 (epoch 84), train_loss = 1.428, time/batch = 0.054\n",
      ">> sample mode:\n",
      "The known the prince,\n",
      "I do both.\n",
      "Have so,\n",
      "Into my husb\n",
      "----------------------------------\n",
      "31905/46375 (epoch 85), train_loss = 1.428, time/batch = 0.073\n",
      ">> sample mode:\n",
      "The Povars do King both,\n",
      "What be twenty your vowed, fo\n",
      "----------------------------------\n",
      "32276/46375 (epoch 86), train_loss = 1.428, time/batch = 0.057\n",
      ">> sample mode:\n",
      "The quarer,\n",
      "I'll be appear? So faith; at mistress' and\n",
      "----------------------------------\n",
      "32647/46375 (epoch 87), train_loss = 1.427, time/batch = 0.050\n",
      ">> sample mode:\n",
      "The know awny of honour\n",
      "In mind: thou, good Warwick, u\n",
      "----------------------------------\n",
      "33018/46375 (epoch 88), train_loss = 1.427, time/batch = 0.058\n",
      ">> sample mode:\n",
      "The what mado's, if the nurse;\n",
      "Exople! We dispersed to\n",
      "----------------------------------\n",
      "33389/46375 (epoch 89), train_loss = 1.427, time/batch = 0.071\n",
      ">> sample mode:\n",
      "The Valan:\n",
      "I'll pray these home itserfied, when cheer \n",
      "----------------------------------\n",
      "33760/46375 (epoch 90), train_loss = 1.427, time/batch = 0.066\n",
      ">> sample mode:\n",
      "The Warwick from Kate,\n",
      "Till I came on thy mind.\n",
      "\n",
      "ANGEL\n",
      "----------------------------------\n",
      "34131/46375 (epoch 91), train_loss = 1.426, time/batch = 0.059\n",
      ">> sample mode:\n",
      "The perceive me; I am lords, wounden: speak with me no\n",
      "----------------------------------\n",
      "34502/46375 (epoch 92), train_loss = 1.426, time/batch = 0.069\n",
      ">> sample mode:\n",
      "The officely, wilt beteropping, oo one, not is please \n",
      "----------------------------------\n",
      "34873/46375 (epoch 93), train_loss = 1.426, time/batch = 0.057\n",
      ">> sample mode:\n",
      "The grave.\n",
      "\n",
      "VINCENTIO:\n",
      "My notiged never.\n",
      "In parlila:\n",
      "T\n",
      "----------------------------------\n",
      "35244/46375 (epoch 94), train_loss = 1.425, time/batch = 0.078\n",
      ">> sample mode:\n",
      "The leaves\n",
      "Nor donest dow heard by his, Mast then will\n",
      "----------------------------------\n",
      "35615/46375 (epoch 95), train_loss = 1.425, time/batch = 0.062\n",
      ">> sample mode:\n",
      "The kepressecal count thy son,\n",
      "'Hent of ammision.\n",
      "\n",
      "QUE\n",
      "----------------------------------\n",
      "35986/46375 (epoch 96), train_loss = 1.425, time/batch = 0.084\n",
      ">> sample mode:\n",
      "The well o' the truth; though cold with oud marrian, t\n",
      "----------------------------------\n",
      "36357/46375 (epoch 97), train_loss = 1.425, time/batch = 0.051\n",
      ">> sample mode:\n",
      "The kill.\n",
      "\n",
      "LEONTES: strike, man like at he hath to one\n",
      "----------------------------------\n",
      "36728/46375 (epoch 98), train_loss = 1.425, time/batch = 0.077\n",
      ">> sample mode:\n",
      "The Oke to your hand: therefore, I mean\n",
      "bildoed by a g\n",
      "----------------------------------\n",
      "37099/46375 (epoch 99), train_loss = 1.424, time/batch = 0.196\n",
      ">> sample mode:\n",
      "The king;\n",
      "For mightisfed, I?\n",
      "\n",
      "VALEUS:\n",
      "All your own at \n",
      "----------------------------------\n",
      "37470/46375 (epoch 100), train_loss = 1.424, time/batch = 0.062\n",
      ">> sample mode:\n",
      "The their sold our oldst lene whole, though I cannot w\n",
      "----------------------------------\n",
      "37841/46375 (epoch 101), train_loss = 1.424, time/batch = 0.071\n",
      ">> sample mode:\n",
      "The were the easit\n",
      "And what said with out-breathame\n",
      "As\n",
      "----------------------------------\n",
      "38212/46375 (epoch 102), train_loss = 1.424, time/batch = 0.076\n",
      ">> sample mode:\n",
      "The master; and be kind's soul to kelforing boy\n",
      "And su\n",
      "----------------------------------\n",
      "38583/46375 (epoch 103), train_loss = 1.424, time/batch = 0.111\n",
      ">> sample mode:\n",
      "The head thy befateful your hagain\n",
      "Letting a will swif\n",
      "----------------------------------\n",
      "38954/46375 (epoch 104), train_loss = 1.423, time/batch = 0.139\n",
      ">> sample mode:\n",
      "The prosence.\n",
      "Being not Sir\n",
      "Goar a poise.\n",
      "\n",
      "AHBILF:\n",
      "He \n",
      "----------------------------------\n",
      "39325/46375 (epoch 105), train_loss = 1.423, time/batch = 0.110\n",
      ">> sample mode:\n",
      "The Edward's child,\n",
      "And what, I am Ned gentlemen you, \n",
      "----------------------------------\n",
      "39696/46375 (epoch 106), train_loss = 1.423, time/batch = 0.062\n",
      ">> sample mode:\n",
      "The leasty that Buck'd home,.\n",
      "\n",
      "BRUTUS:\n",
      "Live you to her\n",
      "----------------------------------\n",
      "40067/46375 (epoch 107), train_loss = 1.423, time/batch = 0.066\n",
      ">> sample mode:\n",
      "The our ass, for what eatsing day;\n",
      "Which more red to t\n",
      "----------------------------------\n",
      "40438/46375 (epoch 108), train_loss = 1.423, time/batch = 0.067\n",
      ">> sample mode:\n",
      "The Enget and in an oath\n",
      "not be bload,\n",
      "And daughter mo\n",
      "----------------------------------\n",
      "40809/46375 (epoch 109), train_loss = 1.423, time/batch = 0.056\n",
      ">> sample mode:\n",
      "The nursed's housafo'st of you?\n",
      "\n",
      "SICINIUS:\n",
      "Ay, my groo\n",
      "----------------------------------\n",
      "41180/46375 (epoch 110), train_loss = 1.423, time/batch = 0.055\n",
      ">> sample mode:\n",
      "The clock is most see our good his father men wheretu \n",
      "----------------------------------\n",
      "41551/46375 (epoch 111), train_loss = 1.422, time/batch = 0.048\n",
      ">> sample mode:\n",
      "The prize\n",
      "Not to coward.\n",
      "\n",
      "ROMEO:\n",
      "Against till my bone \n",
      "----------------------------------\n",
      "41922/46375 (epoch 112), train_loss = 1.422, time/batch = 0.068\n",
      ">> sample mode:\n",
      "The merch as in\n",
      "coalish, onswer and bold, I know, be q\n",
      "----------------------------------\n",
      "42293/46375 (epoch 113), train_loss = 1.422, time/batch = 0.098\n",
      ">> sample mode:\n",
      "The will not hear you make them be feverous\n",
      "upon\n",
      "With \n",
      "----------------------------------\n",
      "42664/46375 (epoch 114), train_loss = 1.422, time/batch = 0.061\n",
      ">> sample mode:\n",
      "The maknown'd worse of it.\n",
      "\n",
      "Servant:\n",
      "Hark, I will no l\n",
      "----------------------------------\n",
      "43035/46375 (epoch 115), train_loss = 1.422, time/batch = 0.057\n",
      ">> sample mode:\n",
      "The proclaim moon,\n",
      "You may desire,\n",
      "Fortune himself mus\n",
      "----------------------------------\n",
      "43406/46375 (epoch 116), train_loss = 1.422, time/batch = 0.075\n",
      ">> sample mode:\n",
      "The wouldly see we giventbed.\n",
      "\n",
      "All:\n",
      "All all's friend t\n",
      "----------------------------------\n",
      "43777/46375 (epoch 117), train_loss = 1.422, time/batch = 0.063\n",
      ">> sample mode:\n",
      "The Fit on thee finds, first bosom of us bear my quant\n",
      "----------------------------------\n",
      "44148/46375 (epoch 118), train_loss = 1.422, time/batch = 0.098\n",
      ">> sample mode:\n",
      "The he now.\n",
      "\n",
      "Third My line to my clock of repul founne\n",
      "----------------------------------\n",
      "44519/46375 (epoch 119), train_loss = 1.422, time/batch = 0.071\n",
      ">> sample mode:\n",
      "The imaging. Threth no rounded?\n",
      "Is tope.\n",
      "Look so dear \n",
      "----------------------------------\n",
      "44890/46375 (epoch 120), train_loss = 1.421, time/batch = 0.060\n",
      ">> sample mode:\n",
      "The knows thy valinable it, indeed: esissare,\n",
      "And find\n",
      "----------------------------------\n",
      "45261/46375 (epoch 121), train_loss = 1.421, time/batch = 0.089\n",
      ">> sample mode:\n",
      "The Eccemple!\n",
      "I late into\n",
      "e'e thee!\n",
      "\n",
      "COMINIUS:\n",
      "Ay? Gun\n",
      "----------------------------------\n",
      "45632/46375 (epoch 122), train_loss = 1.421, time/batch = 0.059\n",
      ">> sample mode:\n",
      "The Rozen;\n",
      "Out, the new a dogethem courtens here secur\n",
      "----------------------------------\n",
      "46003/46375 (epoch 123), train_loss = 1.421, time/batch = 0.054\n",
      ">> sample mode:\n",
      "The hangs in.\n",
      "\n",
      "CAMILLO:\n",
      "When I me she should not it ey\n",
      "----------------------------------\n",
      "46374/46375 (epoch 124), train_loss = 1.421, time/batch = 0.066\n",
      ">> sample mode:\n",
      "The pierce at, our concluck'ss, my life,\n",
      "Be here go to\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(num_epochs): # num_epochs is 5 for test, but should be higher\n",
    "        sess.run(tf.assign(model.lr, learning_rate * (decay_rate ** e)))\n",
    "        data_loader.reset_batch_pointer()\n",
    "        state = sess.run(model.initial_state) # (2x[60x128])\n",
    "        for b in range(data_loader.num_batches): #for each batch\n",
    "            start = time.time()\n",
    "            x, y = data_loader.next_batch()\n",
    "            feed = {model.input_data: x, model.targets: y, model.initial_state:state}\n",
    "            train_loss, state, _ = sess.run([model.cost, model.final_state, model.train_op], feed)\n",
    "            end = time.time()\n",
    "        print(\"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\" \\\n",
    "                .format(e * data_loader.num_batches + b, num_epochs * data_loader.num_batches, e, train_loss, end - start))\n",
    "        with tf.variable_scope(\"rnn\", reuse=True):\n",
    "            sample_model = LSTMModel(sample=True)\n",
    "            print (sample_model.sample(sess, data_loader.chars , data_loader.vocab, num=50, prime='The ', sampling_type=1))\n",
    "            print ('----------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Want to learn more?\n",
    "\n",
    "Running deep learning programs usually needs a high performance platform. PowerAI speeds up deep learning and AI. Built on IBM's Power Systems, PowerAI is a scalable software platform that accelerates deep learning and AI with blazing performance for individual users or enterprises. The PowerAI platform supports popular machine learning libraries and dependencies including Tensorflow, Caffe, Torch, and Theano. You can download a [free version of PowerAI](https://cocl.us/ML0120EN_PAI).\n",
    "\n",
    "Also, you can use Data Science Experience to run these notebooks faster with bigger datasets. Data Science Experience is IBM's leading cloud solution for data scientists, built by data scientists. With Jupyter notebooks, RStudio, Apache Spark and popular libraries pre-packaged in the cloud, DSX enables data scientists to collaborate on their projects without having to install anything. Join the fast-growing community of DSX users today with a free account at [Data Science Experience](https://cocl.us/ML0120EN_DSX)This is the end of this lesson. Hopefully, now you have a deeper and intuitive understanding regarding the LSTM model. Thank you for reading this notebook, and good luck on your studies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thanks for completing this lesson!\n",
    "Created by: <a href = \"https://linkedin.com/in/saeedaghabozorgi\"> Saeed Aghabozorgi </a></h4>\n",
    "This code is based on this [blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), and the code is an step-by-step implimentation of the [character-level implimentation](https://github.com/crazydonkey200/tensorflow-char-rnn)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
