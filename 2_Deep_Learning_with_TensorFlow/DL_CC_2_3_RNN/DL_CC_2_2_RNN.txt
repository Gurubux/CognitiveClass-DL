----------------------------------------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------------------------------------------
MODULE 3 â€“ RECURRENT NEURAL NETWORKS (RNN)
Learning Objective 
	The Recurrent Neural Network Model-(RNN)
	Long Short-Term Memory-(LSTM)
	Recursive Neural Tensor Network Theory-(RNTN)
	Applying Recurrent Networks to Language Modelling

RNN
- Modeling sequential data 
- Stock Market Data
- We simply need to feed the network With the sequential data, it then maintains the context of the data And thus, learns the patterns within the data
- Sentiment analysis
- Language modeling - Predict the next word in a sentence.
- Text translation - Google Translator
- Speech-to-text

----------------------------------------------------------------------------------------------------------------------------------------------------
The Sequential Problem



----------------------------------------------------------------------------------------------------------------------------------------------------
The RNN Model



----------------------------------------------------------------------------------------------------------------------------------------------------
LONG SHORT-TERM MEMORY- (LSTM) model
Long Short-Term Memory Model
The Long Short-Term Memory, as it was called, was an abstraction of how computer memory works. It is "bundled" with whatever processing unit is implemented in the Recurrent Network, although outside of its flow, and is responsible for keeping, reading, and outputting information For the model. The way it works is simple: you have a linear unit, which is the information cell itself, surrounded by three logistic gates responsible For maintaining the data. One gate is For inputting data into the information cell, one is For outputting data From the input cell, and the last one is to keep or forget data depending on the needs of the network.
Thanks to that, it not only solves the problem of keeping states, because the network can choose to forget data whenever information is not needed, it also solves the gradient problems, since the Logistic Gates have a very nice derivative.


LONG SHORT-TERM MEMORY ARCHITECTURE
The Long Short-Term Memory is composed of a linear unit surrounded by three logistic gates. The name for these gates vary from place to place, but the most usual names for them are:
	- the "Input" or "Write" Gate, which handles the writing of data into the information cell
	- the "Output" or "Read" Gate, which handles the sending of data back onto the Recurrent Network
	- the "Keep" or "Forget" Gate, which handles the maintaining and modification of the data stored in the information cell

"https://raw.githubusercontent.com/Gurubux/CognitiveClass-DL/master/2_Deep_Learning_with_TensorFlow/DL_CC_2_3_RNN/LSTM_Components.png"
Diagram of the Long Short-Term Memory Unit

The three gates are the centerpiece of the LSTM unit. The gates, when activated by the network, perform their respective functions. For example, the Input Gate will write whatever data it is passed into the information cell, the Output Gate will return whatever data is in the information cell, and the Keep Gate will maintain the data in the information cell. These gates are analog and multiplicative, and as such, can modify the data based on the signal they are sent.




----------------------------------------------------------------------------------------------------------------------------------------------------
RNN model TO LANGUAGE MODELLING




---------------------------------------------------------------------------------------------------------------
LABS

3.1-Reveiw-LSTM-basics
3.2-Review-LSTM-LanguageModelling
3.3-Review-LSTM-CharacterModelling
3.4-Review-LSTM-MNIST-Database